#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è Cursor IDE –∏ –¥—Ä—É–≥–∏—Ö —Ä–µ–¥–∞–∫—Ç–æ—Ä–æ–≤
"""

import json
import os
import sys
from typing import Dict, List, Any, Optional
from datetime import datetime
from .base_converter import BaseConverter
from abc import abstractmethod

class SplitConverter(BaseConverter):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Ä–∞–∑–±–∏–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"""
    
    def __init__(self, input_file: str, max_file_size_kb: int = 50, max_items_per_file: int = 50, verbose: bool = False):
        super().__init__(input_file, verbose)
        self.max_file_size_kb = max_file_size_kb
        self.max_items_per_file = max_items_per_file
    
    def split_by_category(self, data: List[Dict]) -> Dict[str, List[Dict]]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""
        categories = {}
        for item in data:
            category = item.get('category', 'other')
            if category not in categories:
                categories[category] = []
            categories[category].append(item)
        return categories
    
    def split_into_chunks(self, items: List[Dict]) -> List[List[Dict]]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É"""
        chunks = []
        current_chunk = []
        current_size = 0
        
        for item in items:
            item_size = len(json.dumps(item, ensure_ascii=False))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã
            if (len(current_chunk) >= self.max_items_per_file or 
                current_size + item_size > self.max_file_size_kb * 1024):
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = []
                    current_size = 0
            
            current_chunk.append(item)
            current_size += item_size
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def export_split(self, data: List[Dict], output_dir: str, prefix: str = ""):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ä–∞–∑–±–∏—Ç–æ–º –≤–∏–¥–µ"""
        categories = self.split_by_category(data)
        
        for category, items in categories.items():
            category_dir = os.path.join(output_dir, category)
            os.makedirs(category_dir, exist_ok=True)
            
            chunks = self.split_into_chunks(items)
            
            for i, chunk in enumerate(chunks):
                filename = f"{category}_{i+1:03d}.json"
                if prefix:
                    filename = f"{prefix}_{filename}"
                
                filepath = os.path.join(category_dir, filename)
                self.export_json({"items": chunk, "metadata": {
                    "category": category,
                    "chunk": i+1,
                    "total_chunks": len(chunks),
                    "items_count": len(chunk),
                    "created_at": datetime.now().isoformat()
                }}, filepath)
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            index_file = os.path.join(category_dir, f"{category}_index.json")
            self.export_json({
                "category": category,
                "total_items": len(items),
                "total_chunks": len(chunks),
                "chunks": [f"{category}_{i+1:03d}.json" for i in range(len(chunks))],
                "created_at": datetime.now().isoformat()
            }, index_file)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–≤–æ–¥–∫—É —ç–∫—Å–ø–æ—Ä—Ç–∞ (–µ—Å–ª–∏ –Ω–µ verbose —Ä–µ–∂–∏–º)
        if not self.verbose:
            self.show_export_summary()
    
    def create_main_index(self, output_dir: str, all_items: List[Dict], mode: str):
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—â–∏–π –∏–Ω–¥–µ–∫—Å –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤"""
        categories = self.split_by_category(all_items)
        
        index = {
            "total_items": len(all_items),
            "categories": {},
            "created_at": datetime.now().isoformat(),
            "mode": mode,
            "settings": {
                "max_file_size_kb": self.max_file_size_kb,
                "max_items_per_file": self.max_items_per_file
            }
        }
        
        for category, items in categories.items():
            chunks = self.split_into_chunks(items)
            index["categories"][category] = {
                "items_count": len(items),
                "chunks_count": len(chunks),
                "files": [f"{category}_{i+1:03d}.json" for i in range(len(chunks))]
            }
        
        index_file = os.path.join(output_dir, "main_index.json")
        self.export_json(index, index_file)
    
    def validate_export(self, output_dir: str):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ª–∏–º–∏—Ç–∞–º —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π"""
        print(f"üîç –í–∞–ª–∏–¥–∞—Ü–∏—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤ {output_dir}...")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        total_files = 0
        valid_files = 0
        size_warnings = []
        lines_warnings = []
        
        for root, dirs, files in os.walk(output_dir):
            for file in files:
                if file.endswith('.json') and not file.endswith('_index.json'):
                    file_path = os.path.join(root, file)
                    size_kb = os.path.getsize(file_path) / 1024
                    
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = len(f.readlines())
                    
                    total_files += 1
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã
                    if size_kb > self.max_file_size_kb:
                        size_warnings.append((file_path, size_kb))
                    elif lines > 500:
                        lines_warnings.append((file_path, lines))
                    else:
                        valid_files += 1
        
        # –í—ã–≤–æ–¥–∏–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:")
        print(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {total_files}")
        print(f"   ‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö: {valid_files}")
        print(f"   ‚ö†Ô∏è  –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞: {len(size_warnings)}")
        print(f"   ‚ö†Ô∏è  –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫: {len(lines_warnings)}")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        if size_warnings:
            print(f"\n‚ö†Ô∏è  –§–∞–π–ª—ã —Å –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞ ({self.max_file_size_kb}KB):")
            self._group_warnings(size_warnings, "—Ä–∞–∑–º–µ—Ä", lambda x: x[1])
        
        if lines_warnings:
            print(f"\n‚ö†Ô∏è  –§–∞–π–ª—ã —Å –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ–º —Å—Ç—Ä–æ–∫ (500):")
            self._group_warnings(lines_warnings, "—Å—Ç—Ä–æ–∫–∏", lambda x: x[1])
        
        if not size_warnings and not lines_warnings:
            print(f"‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –ª–∏–º–∏—Ç–∞–º!")
    
    def _group_warnings(self, warnings: List[tuple], warning_type: str, value_extractor):
        """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º"""
        if not warnings:
            return
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories = {}
        for file_path, value in warnings:
            category = file_path.split(os.sep)[-2] if len(file_path.split(os.sep)) > 1 else "unknown"
            if category not in categories:
                categories[category] = []
            categories[category].append((file_path, value))
        
        # –í—ã–≤–æ–¥–∏–º –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category, items in sorted(categories.items()):
            print(f"   üìÅ {category}: {len(items)} —Ñ–∞–π–ª–æ–≤")
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º –∑–Ω–∞—á–µ–Ω–∏–π
            ranges = self._group_by_ranges(items, value_extractor)
            for range_name, range_items in ranges.items():
                if len(range_items) <= 3:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –µ—Å–ª–∏ –∏—Ö –º–∞–ª–æ
                    for file_path, value in range_items:
                        filename = os.path.basename(file_path)
                        print(f"      {filename}: {value:.1f}{'KB' if warning_type == '—Ä–∞–∑–º–µ—Ä' else ''}")
                else:
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –µ—Å–ª–∏ —Ñ–∞–π–ª–æ–≤ –º–Ω–æ–≥–æ
                    avg_value = sum(value_extractor(item) for item in range_items) / len(range_items)
                    print(f"      {range_name}: {len(range_items)} —Ñ–∞–π–ª–æ–≤ (—Å—Ä–µ–¥–Ω–µ–µ: {avg_value:.1f}{'KB' if warning_type == '—Ä–∞–∑–º–µ—Ä' else ''})")
    
    def _group_by_ranges(self, items: List[tuple], value_extractor) -> Dict[str, List[tuple]]:
        """–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º –∑–Ω–∞—á–µ–Ω–∏–π"""
        ranges = {
            "50-60": [],
            "60-70": [],
            "70-80": [],
            "80-90": [],
            "90+": []
        }
        
        for item in items:
            value = value_extractor(item)
            if value <= 60:
                ranges["50-60"].append(item)
            elif value <= 70:
                ranges["60-70"].append(item)
            elif value <= 80:
                ranges["70-80"].append(item)
            elif value <= 90:
                ranges["80-90"].append(item)
            else:
                ranges["90+"].append(item)
        
        # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã
        return {k: v for k, v in ranges.items() if v}
    
    @abstractmethod
    def convert(self) -> None:
        """–ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ - –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∞—Ö"""
        pass 